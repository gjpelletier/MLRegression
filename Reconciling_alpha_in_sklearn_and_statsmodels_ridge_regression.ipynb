{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8905ba27-fbe0-477e-9f39-515cb2689023",
   "metadata": {},
   "source": [
    "# Reconciling alpha in statsmodels and sklearn ridge regression\n",
    "\n",
    "This analysis by Paul Zivich (https://sph.unc.edu/adv_profile/paul-zivich/) explains how to get the same results of ridge regression from statsmodels and sklearn. The difference is that sklearn's Ridge function internally scales the input of the 'alpha' regularization term during excecution as alpha / n_samples where n_samples is the number of samples, compared with statsmodels which does not apply this scaling of the regularization parameter during execution. You can have the ridge implementations match if you re-scale the sklearn input alpha = alpha / n_samples for statsmodels. Note that this rescaling of alpha only applies to ridge regression. The sklearn and statsmodels results for Lasso regression using exactly the same alpha values for input without rescaling.\n",
    "\n",
    "Here is a link to the original post of this analysis by Paul Zivich on stackoverflow.com:  \n",
    "\n",
    "https://stackoverflow.com/questions/72260808/mismatch-between-statsmodels-and-sklearn-ridge-regression\n",
    "\n",
    "While comparing statsmodels and sklearn, Paul found that the two libraries result in different output for ridge regression. Below is an simple example of the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8651acb0-b69d-4eda-b19e-6f6320baeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "np.random.seed(142131)\n",
    "\n",
    "n = 500\n",
    "d = pd.DataFrame()\n",
    "d['A'] = np.random.normal(size=n)\n",
    "d['B'] = d['A'] + np.random.normal(scale=0.25, size=n)\n",
    "d['C'] = np.random.normal(size=n)\n",
    "d['D'] = np.random.normal(size=n)\n",
    "d['intercept'] = 1\n",
    "d['Y'] = 5 - 2*d['A'] + 1*d['D'] + np.random.normal(size=n)\n",
    "\n",
    "y = np.asarray(d['Y'])\n",
    "X = np.asarray(d[['intercept', 'A', 'B', 'C', 'D']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10161b4e-05be-41ac-a2f3-417c5d80fefa",
   "metadata": {},
   "source": [
    "First, using sklearn and ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7821ddb3-ffe8-4874-8212-b794ef62b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge params from sklearn intercept and coefs: \n",
      " 4.997208595888691 [-2.00968258  0.03363013 -0.02144874  1.02895154]\n"
     ]
    }
   ],
   "source": [
    "alpha_sklearn = 1\n",
    "ridge = Ridge(alpha=alpha_sklearn, fit_intercept=True)\n",
    "ridge.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('ridge params from sklearn intercept and coefs: \\n',ridge.intercept_, ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c67a3d-1652-487b-b7a8-6e44ce40dd11",
   "metadata": {},
   "source": [
    "Next, statsmodels and OLS.fit_regularized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ea2cba-b251-4ff4-86a9-5bb8c7d461d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge params from statsmodels: \n",
      " [ 5.01623298e+00 -6.91643749e-01 -6.39008772e-01  1.55825435e-03\n",
      "  5.51575433e-01]\n"
     ]
    }
   ],
   "source": [
    "alpha_statsmodels = np.array([0, 1., 1., 1., 1.])\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=0., alpha=alpha_statsmodels)\n",
    "print('ridge params from statsmodels: \\n',ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90116b-3511-4cf9-a6b5-926db384a9ef",
   "metadata": {},
   "source": [
    "which outputs [5.01623, -0.69164, -0.63901, 0.00156, 0.55158]. However, since these both are implementing ridge regression, Paul expected them to be the same.\n",
    "\n",
    "Note, that neither of these penalize the intercept term (Paul checked that as a possible potential difference). Paul found that statsmodels and sklearn provide the same output for LASSO regression. Below is a demonstration with the previous data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e19d0956-4a4c-4e2e-afaa-05f32cb38b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso params from sklearn intercept and coefs: \n",
      " 5.014649977131442 [-1.5183174  -0.          0.          0.57799164]\n",
      "lasso params from statsmodels: \n",
      " [ 5.01464998 -1.51831729  0.          0.          0.57799166]\n"
     ]
    }
   ],
   "source": [
    "# sklearn LASSO\n",
    "alpha_sklearn = 0.5\n",
    "lasso = Lasso(alpha=alpha_sklearn, fit_intercept=True)\n",
    "lasso.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('lasso params from sklearn intercept and coefs: \\n',lasso.intercept_, lasso.coef_)\n",
    "\n",
    "# statsmodels LASSO\n",
    "alpha_statsmodels = np.array([0, 0.5, 0.5, 0.5, 0.5])\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=1., alpha=alpha_statsmodels)\n",
    "print('lasso params from statsmodels: \\n',ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f644c31-6f87-42e4-a485-34e765c3cb26",
   "metadata": {},
   "source": [
    "which both output [5.01465, -1.51832, 0., 0., 0.57799].\n",
    "\n",
    "So Paul's question is why do the estimated coefficients for ridge regression differ across implementations in sklearn and statsmodels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529eecd-1a1b-4c1f-b62a-a6df52c61dbc",
   "metadata": {},
   "source": [
    "After digging around a little more, Paul discovered the answer by trial and error as to why the statsmodels and sklearn ridge regression results differ. The difference is that sklearn's Ridge scales the regularization term as alpha_scaled = alpha_input / n where n is the number of observations and alpha_input is the input argument values of alpha used with sklearn. statsmodels does not apply this scaling of the regularization parameter. You can have the statsmodels and sklearn ridge implementations match if you re-scale the regularizaiton parameter used for input to sklearn when you prepare the input required for statsmodels.\n",
    "\n",
    "In other words, if you use the following input values of alpha for sklearn:\n",
    "\n",
    "alpha_sklearn = 1\n",
    "\n",
    "then you would need to use the following input of alpha=alpha_scaled when using statsmodels to get the same result:\n",
    "\n",
    "alpha_statsmodels = alpha_sklearn / n_samples\n",
    "\n",
    "where n_samples is the number of samples (n_samples = X.shape[0]).\n",
    "\n",
    "Using Paul's posted example, here is how you would have the output of ridge regression parameters match between the statsmodels and sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c303aa11-82df-4e3b-b3b6-6dea16217e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge params from sklearn intercept and coefs: \n",
      " 4.997208595888691 [-2.00968258  0.03363013 -0.02144874  1.02895154]\n",
      "ridge params from statsmodels with alpha=alpha/n: \n",
      " [ 4.9972086  -2.00968258  0.03363013 -0.02144874  1.02895154]\n"
     ]
    }
   ],
   "source": [
    "# sklearn \n",
    "# NOTE: there is no difference from above\n",
    "alpha_sklearn = 1\n",
    "ridge = Ridge(alpha=alpha_sklearn, fit_intercept=True)\n",
    "ridge.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('ridge params from sklearn intercept and coefs: \\n',ridge.intercept_, ridge.coef_)\n",
    "\n",
    "# statsmodels\n",
    "# NOTE: going to re-scale the regularization parameter based on n observations\n",
    "n_samples = X.shape[0]\n",
    "alpha_statsmodels = np.array([0, 1., 1., 1., 1.]) / n_samples  # scaling of alpha by n\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=0., alpha=alpha_statsmodels)\n",
    "print('ridge params from statsmodels with alpha=alpha/n: \\n',ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2673021-d0af-4b40-be22-da1b589f8975",
   "metadata": {},
   "source": [
    "Now both output [ 4.99721, -2.00968, 0.03363, -0.02145, 1.02895].\n",
    "\n",
    "Paul posted this analysis in the hopes that if someone else is in the same situation trying to match resuts of ridge regression using statsmodels and sklearn, they can find the answer more easily (since Paul had not seen any discussion of this difference before). It is also noteworthy that sklearn's Ridge re-scales the tuning parameter but sklearn's Lasso does not. Paul was not able to find an explanation of this behaviour in the sklearn documentation for Ridge and LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd7ff4-5f27-434f-9efd-6d8088ff739a",
   "metadata": {},
   "source": [
    "### Variance Inflation Factors for ridge regression\n",
    "\n",
    "Josef Perktold wrote the followng function to calculate VIF for ridge regression:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1956fb27-1a8b-4abf-922d-baef321df722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_ridge(X, pen_factors, is_corr=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Variance Inflation Factor for Ridge regression \n",
    "\n",
    "    adapted from statsmodels function vif_ridge by Josef Perktold https://gist.github.com/josef-pkt\n",
    "    source: https://github.com/statsmodels/statsmodels/issues/1669\n",
    "    source: https://stackoverflow.com/questions/23660120/variance-inflation-factor-in-ridge-regression-in-python\n",
    "    author: https://stackoverflow.com/users/333700/josef\n",
    "    Josef is statsmodels maintainer and developer, semi-retired from scipy.stats maintainance\n",
    "\n",
    "    assumes penalization is on standardized feature variables\n",
    "    assumes alpha is scaled by n_samples in calc of penalty factors if using sklearn Ridge (see note below)\n",
    "    data should not include a constant\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like with dimension n_samples x n_features\n",
    "        correlation matrix if is_corr=True or standardized feature data if is_corr is False.\n",
    "    pen_factors : iterable array of of Ridge penalization factors with dimension n_alpha x n_coef, \n",
    "        where in a fitted Ridge model at each alpha in an iterable vector:\n",
    "            pen_factor = alpha * np.sum(coefficients ** 2)\n",
    "        where coefficients is the array of coefficients at each alpha\n",
    "        excluding the intercept, and \n",
    "            alpha = alpha_input / n_samples (if using sklearn Ridge), or\n",
    "            alpha = alpha_input (if using statsmodels ridge with OLS.fit_regularized)\n",
    "        where alpha_input is the input argument of alpha used \n",
    "        with either sklearn or statsmodels, depending on whichever you are using\n",
    "        (see explanation in note below for difference between sklearn and statsmodels)\n",
    "    is_corr : bool\n",
    "        Boolean to indicate how corr_x is interpreted, see corr_x\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vif : ndarray\n",
    "        variance inflation factors for parameters in columns and \n",
    "        ridge penalization factors in rows\n",
    "\n",
    "    could be optimized for repeated calculations\n",
    "\n",
    "    Note about scaling of alpha in statsmodels vs sklearn \n",
    "    -------\n",
    "    An analysis by Paul Zivich (https://sph.unc.edu/adv_profile/paul-zivich/) explains \n",
    "    how to get the same results of ridge regression from statsmodels and sklearn. \n",
    "    The difference is that sklearn's Ridge function scales the input of the 'alpha' \n",
    "    regularization term during excecution as alpha / n where n is the number of observations, \n",
    "    compared with statsmodels which does not apply this scaling of the regularization \n",
    "    parameter during execution. You can have the ridge implementations match \n",
    "    if you re-scale the sklearn input alpha = alpha / n for statsmodels. \n",
    "    Note that this rescaling of alpha only applies to ridge regression. \n",
    "    The sklearn and statsmodels results for Lasso regression using exactly \n",
    "    the same alpha values for input without rescaling.\n",
    "    \n",
    "    Here is a link to the original post of this analysis by Paul Zivich:\n",
    "    \n",
    "    https://stackoverflow.com/questions/72260808/mismatch-between-statsmodels-and-sklearn-ridge-regression\n",
    "    \n",
    "    Therefore, since this vif_ridge function was developed for statsmodels,\n",
    "    and if you are using this function with the sklearn Ridge functions,\n",
    "    you will need to calculate the penalty factors as follows:\n",
    "\n",
    "    pen_factor = alpha / n_samples * (coefficients ** 2)\n",
    "    \n",
    "    To scale alpha by the number of samples using sklearn, \n",
    "    you would need to do it manually, like this:\n",
    "    \n",
    "    from sklearn.linear_model import Ridge    \n",
    "    n_samples = X.shape[0]\n",
    "    scaled_alpha = alpha / n_samples\n",
    "    ridge = Ridge(alpha=scaled_alpha)\n",
    "    ridge.fit(X, y)\n",
    "\n",
    "    Example calculation of pen_factors and use of vif_ridge (statsmodels function)\n",
    "    -------\n",
    "    from sklearn.datasets import make_regression\n",
    "    X, y, w = make_regression(\n",
    "        n_samples=100, n_features=10, n_informative=8, coef=True, random_state=1)\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    clf = Ridge()\n",
    "    # Generate values for `alpha` that are evenly distributed on a logarithmic scale\n",
    "    n_samples = X.shape[0]\n",
    "    alphas = np.logspace(-3, 4, 200)\n",
    "    coefs = []\n",
    "    errors_coefs = []\n",
    "    pen_factors = []\n",
    "    # Train the model with different regularisation strengths\n",
    "    for a in alphas:\n",
    "        clf.set_params(alpha=a).fit(X, y)\n",
    "        coefs.append(clf.coef_)\n",
    "        errors_coefs.append(mean_squared_error(clf.coef_, w))\n",
    "        # Extract coefficients (w)\n",
    "        coefficients = clf.coef_\n",
    "        # Calculate the penalty term\n",
    "        # pen_factors.append(a * np.sum(coefficients ** 2))\n",
    "        # NOTE:\n",
    "        # sklearn ridge scales input alpha during execution by dividing by n_samples to calculate penalty factor\n",
    "        # statsmodels ridge regression does not scale input alpha by n_samples during execution\n",
    "        # https://stackoverflow.com/questions/72260808/mismatch-between-statsmodels-and-sklearn-ridge-regression\n",
    "        # uncomment one of the following two lines depending on if you are using sklearn or statsmodels\n",
    "        pen_factors.append(a / n_samples * np.sum(clf.coef_ ** 2))     # if using sklearn for ridge regression\n",
    "        # pen_factors.append(a * np.sum(clf.coef_ ** 2))           # if using statsmodels for ridge regression\n",
    "    # dataframe of the series of VIF values corresponding the penalty factors at each value of alpha\n",
    "    vifs = pd.DataFrame(vif_ridge(X, pen_factors))\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    if not is_corr:\n",
    "        corr = np.corrcoef(X, rowvar=0, bias=True)\n",
    "    else:\n",
    "        corr = X\n",
    "\n",
    "    eye = np.eye(corr.shape[1])\n",
    "    res = []\n",
    "    for k in pen_factors:\n",
    "        minv = np.linalg.inv(corr + k * eye)\n",
    "        vif = minv.dot(corr).dot(minv)\n",
    "        res.append(np.diag(vif))\n",
    "\n",
    "    return np.asarray(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132a562-36fe-4371-925f-59152ae59f7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8c649f1-92af-4dc4-8a8a-a1267f0580dc",
   "metadata": {},
   "source": [
    "# Note to Josef Perktold regarding use of vif_ridge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a7e38-0ee6-4a4e-90a1-a8e2f5fcbe0a",
   "metadata": {},
   "source": [
    "Hi Josef,\n",
    "\n",
    "For example, if I use the following input data for X and y:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "np.random.seed(142131)\n",
    "n = 500\n",
    "d = pd.DataFrame()\n",
    "d['A'] = np.random.normal(size=n)\n",
    "d['B'] = d['A'] + np.random.normal(scale=0.25, size=n)\n",
    "d['C'] = np.random.normal(size=n)\n",
    "d['D'] = np.random.normal(size=n)\n",
    "d['intercept'] = 1\n",
    "d['Y'] = 5 - 2*d['A'] + 1*d['D'] + np.random.normal(size=n)\n",
    "y = np.asarray(d['Y'])\n",
    "X = np.asarray(d[['intercept', 'A', 'B', 'C', 'D']])\n",
    "```\n",
    "\n",
    "If I want to get the same ridge regression results using sklearn and statsmodels, then I have to use different input values of alpha for each to get the same output of rigde regression coefficients as shown in the following code:\n",
    "\n",
    "```\n",
    "# use sklearn Ridge\n",
    "alpha_sklearn = 1\n",
    "ridge = Ridge(alpha=alpha_sklearn, fit_intercept=True)\n",
    "ridge.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('output params from sklearn with alpha = 1: \\n',ridge.intercept_, ridge.coef_)\n",
    "\n",
    "# use statsmodels OLS fit_regularized L1_wt=0\n",
    "# NOTE: we need to re-scale the alpha parameter used in sklearn to get samee results in statsmodels\n",
    "n_samples = X.shape[0]\n",
    "alpha_statsmodels = np.array([0, 1., 1., 1., 1.]) / n_samples  # scaling of alpha by n\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=0., alpha=alpha_statsmodels)\n",
    "print('output params from statsmodels with alpha = 1 / n_samples: \\n',ols.params)\n",
    "```\n",
    "\n",
    "This results in the following output of ridge regression coefficients from sklearn and statsmodels\n",
    "\n",
    "```\n",
    "ridge params from sklearn intercept and coefs: \n",
    " 4.997208595888691 [-2.00968258  0.03363013 -0.02144874  1.02895154]\n",
    "ridge params from statsmodels with alpha=alpha/n: \n",
    " [ 4.9972086  -2.00968258  0.03363013 -0.02144874  1.02895154]\n",
    "```\n",
    "\n",
    "In other words, if I use an input value of alpha=1 with sklearn, then I need to use an input value of alpha=1/n_samples with statsmodels if I want to get the same results for ridge regression.\n",
    "\n",
    "Based on this behavior, it appears that the sklearn implementation of ridge regression is internally scaling the alpha parameter by n_samples. Therefore, I am thinking that if I am using your vif_ridge function with sklearn for my analysis, then the input values of pen_factors for vif_ridge should be as follows:\n",
    "\n",
    "pen_factors = alpha / n_samples\n",
    "\n",
    "where alpha is the input value that I use for the sklearn Ridge function, and n_samples is X.shape[0].\n",
    "\n",
    "Does that approach look correct to you?\n",
    "\n",
    "Best,\n",
    "\n",
    "Greg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432cb4e3-04dd-42e1-9c28-37c54d89e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "np.random.seed(142131)\n",
    "n = 500\n",
    "d = pd.DataFrame()\n",
    "d['A'] = np.random.normal(size=n)\n",
    "d['B'] = d['A'] + np.random.normal(scale=0.25, size=n)\n",
    "d['C'] = np.random.normal(size=n)\n",
    "d['D'] = np.random.normal(size=n)\n",
    "d['intercept'] = 1\n",
    "d['Y'] = 5 - 2*d['A'] + 1*d['D'] + np.random.normal(size=n)\n",
    "y = np.asarray(d['Y'])\n",
    "X = np.asarray(d[['intercept', 'A', 'B', 'C', 'D']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990b634f-1d81-44aa-817b-52d300a18a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output params from sklearn with alpha = 1: \n",
      " 4.997208595888691 [-2.00968258  0.03363013 -0.02144874  1.02895154]\n",
      "output params from statsmodels with alpha = 1 / n_samples: \n",
      " [ 4.9972086  -2.00968258  0.03363013 -0.02144874  1.02895154]\n"
     ]
    }
   ],
   "source": [
    "# use sklearn Ridge\n",
    "alpha_sklearn = 1\n",
    "ridge = Ridge(alpha=alpha_sklearn, fit_intercept=True)\n",
    "ridge.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('output params from sklearn with alpha = 1: \\n',ridge.intercept_, ridge.coef_)\n",
    "\n",
    "# use statsmodels OLS fit_regularized L1_wt=0\n",
    "# NOTE: we need to re-scale the alpha parameter used in sklearn to get samee results in statsmodels\n",
    "n_samples = X.shape[0]\n",
    "alpha_statsmodels = np.array([0, 1., 1., 1., 1.]) / n_samples  # scaling of alpha by n\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=0., alpha=alpha_statsmodels)\n",
    "print('output params from statsmodels with alpha = 1 / n_samples: \\n',ols.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0984ee-9427-4dab-9e90-54c6cf3e8e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
